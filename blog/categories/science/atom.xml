<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: science | Strangely Specific and Very Odd]]></title>
  <link href="http://Gastove.github.io/blog/categories/science/atom.xml" rel="self"/>
  <link href="http://Gastove.github.io/"/>
  <updated>2013-09-15T08:48:47-07:00</updated>
  <id>http://Gastove.github.io/</id>
  <author>
    <name><![CDATA[Ross M. Donaldson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bloody Syntax: Standardizing on data.table in R]]></title>
    <link href="http://Gastove.github.io/blog/2013/09/14/bloody-syntax-standardizing-on-data-dot-table-in-r/"/>
    <updated>2013-09-14T11:54:00-07:00</updated>
    <id>http://Gastove.github.io/blog/2013/09/14/bloody-syntax-standardizing-on-data-dot-table-in-r</id>
    <content type="html"><![CDATA[<p>The amount of data in my work, like so many things, creeps. I show a chart of five things, people ask for the other six. I show two weeks of history, I’m asked for four more. I instrument a feature and then it’s deployed on six new platforms and how’s it performing, for all 11 things, with six weeks of history?</p>

<p>There’s plenty of good news. Conceptually, processing two gigabytes of data isn’t any different than processing one; doubling the amount of history displayed in a chart raises questions of clarity and data display, but on its face, little else. With… one or two exceptions, all of which have to do with performance. This bites you from two places:</p>

<ol>
  <li>The Data Store: unless you’re working with something like Hadoop, it’s unlikely you’ll be able to do much cleanup on your data before it leaves the datastore. MySQL lacks… basically everything you could ever want, and probably can’t perform meaningful aggregations in MySQL anyways. The latest version of Hive supports windowing functions, which can – broadly speaking – be used for outlier cleanup. Me personally? I’m not on the latest version. Conclusion: data must be aggregated after the dataset is pulled.</li>
  <li>Development: do you work in R? I sure do. Did you know that R can’t think about anything it can’t hold in active memory? No paging, cool as paging is, and object serialization can, frankly, be a pain in the arse. A cool side-effect of R’s memory limit is that there’s only one R Session process, and <em>that</em> process is limited by system architecture. A 32-bit system can only allocate four-gigabytes-ish of memory, minus whatever the OS itself uses (maybe up to a gig?). Now, suddenly, you have a maximum total workspace for all objects and processes of… about three gigabytes. It’s amazing how fast that goes away.</li>
</ol>

<p>So! Time to crunch a file that occupies 1.2gb on disk. Many R operations – especially those on data.frames – create full copies of the source data as they act on them. This is a pernicious problem; data frames are ubiquitous, and incredibly useful. For instance: much as I love <a href="http://plyr.had.co.nz/">plyr</a>, memory efficient… it is not, especially if you want to parallelize the operation across more than one processor core. Now what do you do?</p>

<p>My answer, grudgingly, is the <a href="http://datatable.r-forge.r-project.org/">data.table</a>. Let’s start with some good news:</p>

<ol>
  <li><code>data.table</code> is fast. I wont reproduce the timing tables from the manual, but the web page asserts: 10+ times faster than <code>tapply()</code>; 100+ times faster than <code>==</code>; 500+ times faster than <code>DF[i,j]&lt;-v alue</code>. So far, in terms of speed, it’s everything I’ve ever dreamed of and a bag of chips.</li>
  <li>It’s gentler, active-memory-wise. Many <code>data.table</code> operations are optimized to act <em>on the extant object</em> rather than by copying it and modifying the copy. Many <code>data.table</code> operations return a new, updated version of the same table. This is very good.</li>
  <li><code>data.table</code> <em>extends</em> <code>data.frame</code>. So, you can use a <code>data.table</code> approximately anywhere you can use a <code>data.frame</code>.</li>
</ol>

<p>Let’s dwell on that last one for a moment. <code>data.table</code> is nice because it gives you exceptionally brisk versions of many of your favorite <code>data.frame</code> operations – like <code>summary</code>. <code>subset</code> is implemented for <code>data.table</code>, that’s nice. But if you’re like me and write functions that ingest <code>data.frames</code>, you’re in for a little heartache.</p>

<p>Take subsetting. In a <code>data.frame</code>, you might do something like <code>df2 &lt;- df[df$foo == 'a', ]</code> – or, to go by rows and columns, <code>df2 &lt;- df[df$foo == 'a', c('col1', 'col2')]</code>. This metaphor is familiar to R; first, we evaluate true/false on all the rows, then we return all the rows that evaluate true, subsetted by a vector of quoted column names.</p>

<p>In a <code>data.table</code>: <code>dt2 &lt;- dt[foo == 'a', list(col1, col2)]</code>.</p>

<p><code>data.table</code> documentation refers to this as <code>dt[i, j]</code> syntax, where <code>i</code> and <code>j</code> are full-blown expressions, evaluated in much the way we’re accustomed to with <code>within(df, ...)</code>. Behavior can be problematic:</p>

<p><code>r
dt[dt$foo == 'a'] # Returns exactly what you'd expect -- rows where dt$foo == 'a' evaluates to `TRUE`
dt[dt$foo == 'a', ] # Also fine!
dt[foo == 'a', 'bar'] # Returns... 'bar'! The string literal, 'bar'. Huzzah.
</code></p>

<p>So that’s a mess, especially if you’re modifying an existing subsetting operation. The flip side:</p>

<p>```r
# The data.frame way
cond1 &lt;- expression(df$foo == var1)
cond2 &lt;- expression(df$bar == var2)
df2 &lt;- df[eval(cond1) &amp; eval(cond2), ]</p>

<h1 id="the-datatable-way">The data.table way</h1>
<p>dt2 &lt;- dt[foo == var1 &amp; bar == var2]
‘’’</p>

<p>This is the advantage of <code>i</code> and <code>j</code> being expressions already – you can simply put everything in place, variables and all, and it evaluates.</p>

<p>Assigning new columns is where things really get good, but also… weird looking.</p>

<p>```r
# Assign a single new column
dt[, new.col := mean(col1)]</p>

<h1 id="multiple-assigns-at-once">Multiple assigns at once</h1>
<p>dt[
,
‘:=’(
    new.col.1 = val1,
    new.col.2 = val2
    )
]</p>

<h1 id="and-the-coup-de-grace-assign-with-aggregate">And the coup de grace, assign with aggregate:</h1>
<p>dt[
,
‘:=’(
    new.col.1 = sum(col1),
    new.col.2 = mean(col2)
    )
by=list(col1, col2)
]
```</p>
]]></content>
  </entry>
  
</feed>
